name: Generate Reuters Technology RSS

on:
  workflow_dispatch: {}
  schedule:
    - cron: '*/20 * * * *'  # every 20 minutes

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout repository
      - name: Checkout repository
        uses: actions/checkout@v3

      # Step 2: Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      # Step 3: Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 feedgen python-dateutil

      # Step 4: Generate RSS feed with rolling history
      - name: Generate RSS feed with history
        run: |
          python << 'EOF'
          import requests, datetime, os
          from bs4 import BeautifulSoup
          from feedgen.feed import FeedGenerator
          from dateutil.parser import parse
          import xml.etree.ElementTree as ET

          feed_file = 'feed.xml'
          max_items = 50

          fg = FeedGenerator()
          fg.title('Reuters Technology RSS')
          fg.link(href='https://rebus77.github.io/r_tech/')
          fg.description('RSS feed for Reuters Technology news')

          # Load existing feed items
          existing_items = []
          if os.path.exists(feed_file):
              tree = ET.parse(feed_file)
              root = tree.getroot()
              for item in root.findall('channel/item'):
                  existing_items.append({
                      'title': item.find('title').text,
                      'link': item.find('link').text,
                      'pubDate': item.find('pubDate').text
                  })

          # Fetch new articles
          url = 'https://www.reuters.com/technology/'
          r = requests.get(url)
          soup = BeautifulSoup(r.text, 'html.parser')

          new_articles = []
          articles = soup.find_all("div", attrs={"data-testid": "story"})
          for item in articles[:15]:
              a_tag = item.find("a")
              if not a_tag:
                  continue
              link = "https://www.reuters.com" + a_tag.get("href", "")
              title = a_tag.get_text(strip=True)
              time_tag = item.find("time")
              pub_dt = parse(time_tag["datetime"]) if time_tag else datetime.datetime.utcnow()
              new_articles.append({'title': title, 'link': link, 'pubDate': pub_dt.isoformat()})

          # Combine new + existing, removing duplicates
          combined = new_articles + [x for x in existing_items if x['link'] not in {a['link'] for a in new_articles}]
          combined = combined[:max_items]  # keep only latest max_items

          # Add items to feed
          for article in combined:
              fe = fg.add_entry()
              fe.id(article['link'])
              fe.link(href=article['link'])
              fe.title(article['title'])
              fe.pubDate(article['pubDate'])

          fg.rss_file(feed_file)
          EOF

      # Step 5: Commit and push RSS feed
      - name: Commit and push RSS feed
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "actions@github.com"
          git add feed.xml
          git commit -m "Update RSS feed"
          git push https://x-access-token:${{ secrets.GH_TOKEN }}@github.com/${{ github.repository }}.git HEAD:main || echo "No changes to commit"
