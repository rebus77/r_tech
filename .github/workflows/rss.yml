name: Generate Reuters Technology RSS

on:
  workflow_dispatch: {}
  schedule:
    - cron: '*/20 * * * *'  # every 20 minutes

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      # 1. Checkout repository
      - name: Checkout repository
        uses: actions/checkout@v3

      # 2. Setup Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      # 3. Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 feedgen python-dateutil

      # 4. Generate RSS feed
      - name: Generate RSS feed
        run: |
          python << 'EOF'
          import requests
          from bs4 import BeautifulSoup
          from feedgen.feed import FeedGenerator
          from dateutil.parser import parse
          import datetime
          import os
          import xml.etree.ElementTree as ET

          feed_file = 'feed.xml'
          max_items = 50

          fg = FeedGenerator()
          fg.title('Reuters Technology RSS')
          fg.link(href='https://rebus77.github.io/r_tech/')
          fg.description('RSS feed for Reuters Technology news')

          # Load existing feed items
          existing_items = []
          if os.path.exists(feed_file):
              tree = ET.parse(feed_file)
              root = tree.getroot()
              for item in root.findall('channel/item'):
                  existing_items.append({
                      'title': item.find('title').text,
                      'link': item.find('link').text,
                      'pubDate': item.find('pubDate').text
                  })

          # Fetch articles
          url = 'https://www.reuters.com/technology/'
          headers = {'User-Agent': 'Mozilla/5.0'}
          r = requests.get(url, headers=headers)
          soup = BeautifulSoup(r.text, 'html.parser')

          new_articles = []

          # Look for top stories, Reuters may change HTML frequently
          articles = soup.select('article.story, div[data-testid="story"]')  # multiple selectors
          for item in articles[:15]:
              a_tag = item.find('a')
              if not a_tag:
                  continue
              link = a_tag.get('href')
              if link.startswith('/'):
                  link = 'https://www.reuters.com' + link
              title = a_tag.get_text(strip=True)
              time_tag = item.find('time')
              if time_tag and time_tag.has_attr('datetime'):
                  pub_dt = parse(time_tag['datetime']).isoformat()
              else:
                  pub_dt = datetime.datetime.utcnow().isoformat()
              new_articles.append({'title': title, 'link': link, 'pubDate': pub_dt})

          # Combine new and existing, remove duplicates
          combined = new_articles + [x for x in existing_items if x['link'] not in {a['link'] for a in new_articles}]
          combined = combined[:max_items]  # keep only latest max_items

          # Add items to feed
          for article in combined:
              fe = fg.add_entry()
              fe.id(article['link'])
              fe.link(href=article['link'])
              fe.title(article['title'])
              fe.pubDate(article['pubDate'])

          # Write feed
          fg.rss_file(feed_file)
          EOF

      # 5. Commit and push
      - name: Commit and push RSS feed
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "actions@github.com"
          git add feed.xml
          git commit -m "Update RSS feed"
          git push https://x-access-token:${{ secrets.GH_TOKEN }}@github.com/${{ github.repository }}.git HEAD:main || echo "No changes to commit"
