name: Generate Reuters Technology RSS

on:
  workflow_dispatch: {}
  schedule:
    - cron: '*/20 * * * *'  # every 20 minutes

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      # 1. Checkout repository
      - name: Checkout repository
        uses: actions/checkout@v3

      # 2. Setup Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      # 3. Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 feedgen python-dateutil lxml

      # 4. Generate RSS feed
      - name: Generate RSS feed
        run: |
          python << 'EOF'
          import requests
          from bs4 import BeautifulSoup
          from feedgen.feed import FeedGenerator
          from dateutil.parser import parse
          import datetime
          import os
          import xml.etree.ElementTree as ET

          feed_file = 'feed.xml'
          max_items = 50

          fg = FeedGenerator()
          fg.title('Reuters Technology RSS')
          fg.link(href='https://rebus77.github.io/r_tech/')
          fg.description('RSS feed for Reuters Technology news')

          # Load existing feed items
          existing_items = []
          existing_links = set()
          if os.path.exists(feed_file):
              tree = ET.parse(feed_file)
              root = tree.getroot()
              for item in root.findall('channel/item'):
                  link = item.find('link').text
                  pub_date_text = item.find('pubDate').text
                  pub_date = parse(pub_date_text) if pub_date_text else datetime.datetime.utcnow()
                  existing_items.append({
                      'title': item.find('title').text,
                      'link': link,
                      'description': item.find('description').text if item.find('description') is not None else '',
                      'pubDate': pub_date
                  })
                  existing_links.add(link)

          new_articles = []

          # --- Try JSON API first ---
          json_url = "https://www.reuters.com/pf/api/v3/content/fetch/articles-by-section?section=technology"
          headers = {
              'User-Agent': 'Mozilla/5.0',
              'Accept': 'application/json',
              'Referer': 'https://www.reuters.com/technology/'
          }

          try:
              r = requests.get(json_url, headers=headers, timeout=10)
              r.raise_for_status()
              data = r.json()
              for article in data.get('result', {}).get('articles', [])[:20]:
                  link = "https://www.reuters.com" + article['canonical_url']
                  if link in existing_links:
                      continue
                  title = article.get('title', 'No Title')
                  description = article.get('summary', '')
                  pub_dt = parse(article['published_time']) if article.get('published_time') else datetime.datetime.utcnow()
                  new_articles.append({
                      'title': title,
                      'link': link,
                      'description': description,
                      'pubDate': pub_dt
                  })
          except Exception as e:
              print(f"JSON API failed, falling back to HTML scraping: {e}")
              html_url = "https://www.reuters.com/technology/"
              r = requests.get(html_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
              soup = BeautifulSoup(r.text, 'lxml')
              articles = soup.select('article, div[data-testid="story"]')[:20]
              for item in articles:
                  a_tag = item.find('a', href=True)
                  if not a_tag:
                      continue
                  link = a_tag['href']
                  if link.startswith('/'):
                      link = 'https://www.reuters.com' + link
                  if link in existing_links:
                      continue
                  title_tag = item.select_one('h3, h2')
                  title = title_tag.get_text(strip=True) if title_tag else a_tag.get_text(strip=True)
                  desc_tag = item.select_one('p')
                  description = desc_tag.get_text(strip=True) if desc_tag else ''
                  time_tag = item.find('time')
                  if time_tag and time_tag.has_attr('datetime'):
                      pub_dt = parse(time_tag['datetime'])
                  else:
                      pub_dt = datetime.datetime.utcnow()
                  new_articles.append({
                      'title': title,
                      'link': link,
                      'description': description,
                      'pubDate': pub_dt
                  })

          # Combine new and existing, sort by pubDate descending
          combined = new_articles + existing_items
          combined.sort(key=lambda x: x['pubDate'], reverse=True)
          combined = combined[:max_items]

          # Add entries to feed
          for article in combined:
              fe = fg.add_entry()
              fe.id(article['link'])
              fe.link(href=article['link'])
              fe.title(article['title'])
              fe.description(article['description'])
              fe.pubDate(article['pubDate'].strftime("%a, %d %b %Y %H:%M:%S %z"))

          fg.rss_file(feed_file)
          EOF

      # 5. Commit and push
      - name: Commit and push RSS feed
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "actions@github.com"
          git add feed.xml
          git commit -m "Update RSS feed" || echo "No changes to commit"
          git push https://x-access-token:${{ secrets.GH_TOKEN }}@github.com/${{ github.repository }}.git HEAD:main
