name: Generate Reuters Technology RSS

on:
  workflow_dispatch: {}
  schedule:
    - cron: '*/20 * * * *'  # every 20 minutes

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      # 1. Checkout repository
      - name: Checkout repository
        uses: actions/checkout@v3

      # 2. Setup Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      # 3. Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 feedgen python-dateutil lxml

      # 4. Generate RSS feed
      - name: Generate RSS feed
        run: |
          python << 'EOF'
          import requests
          from bs4 import BeautifulSoup
          from feedgen.feed import FeedGenerator
          from dateutil.parser import parse
          import datetime
          import os
          import json

          feed_file = 'feed.xml'
          seen_file = 'seen.json'
          max_items = 50

          # Load seen URLs
          if os.path.exists(seen_file):
              with open(seen_file, 'r') as f:
                  seen_urls = set(json.load(f))
          else:
              seen_urls = set()

          fg = FeedGenerator()
          fg.title('Reuters Technology RSS')
          fg.link(href='https://rebus77.github.io/r_tech/')
          fg.description('Custom RSS feed for Reuters Technology news')

          # Load existing feed items
          existing_items = []
          if os.path.exists(feed_file):
              import xml.etree.ElementTree as ET
              tree = ET.parse(feed_file)
              root = tree.getroot()
              for item in root.findall('channel/item'):
                  link = item.find('link').text
                  existing_items.append({
                      'title': item.find('title').text,
                      'link': link,
                      'description': item.find('description').text if item.find('description') is not None else '',
                      'pubDate': item.find('pubDate').text
                  })
                  seen_urls.add(link)

          # Fetch articles
          url = 'https://www.reuters.com/technology/'
          headers = {'User-Agent': 'Mozilla/5.0'}
          r = requests.get(url, headers=headers)
          r.raise_for_status()
          soup = BeautifulSoup(r.text, 'lxml')

          new_articles = []

          # Look for article blocks
          articles = soup.select('article, div[data-testid="story"]')[:20]  # top 20

          for item in articles:
              a_tag = item.find('a', href=True)
              if not a_tag:
                  continue
              link = a_tag['href']
              if link.startswith('/'):
                  link = 'https://www.reuters.com' + link
              if link in seen_urls:
                  continue  # skip already seen
              # Title
              title_tag = item.select_one('h3, h2')
              title = title_tag.get_text(strip=True) if title_tag else a_tag.get_text(strip=True)
              # Description
              desc_tag = item.select_one('p')
              description = desc_tag.get_text(strip=True) if desc_tag else ''
              # Publication date
              time_tag = item.find('time')
              if time_tag and time_tag.has_attr('datetime'):
                  pub_dt = parse(time_tag['datetime']).strftime("%a, %d %b %Y %H:%M:%S %z")
              else:
                  pub_dt = datetime.datetime.utcnow().strftime("%a, %d %b %Y %H:%M:%S +0000")
              new_articles.append({
                  'title': title,
                  'link': link,
                  'description': description,
                  'pubDate': pub_dt
              })
              seen_urls.add(link)

          # Merge new articles with existing, keeping max_items
          combined = new_articles + existing_items
          combined = combined[:max_items]

          # Add items to feed
          for article in combined:
              fe = fg.add_entry()
              fe.id(article['link'])
              fe.link(href=article['link'])
              fe.title(article['title'])
              fe.description(article['description'])
              fe.pubDate(article['pubDate'])

          # Write feed
          fg.rss_file(feed_file)

          # Save updated seen URLs
          with open(seen_file, 'w') as f:
              json.dump(list(seen_urls), f)
          EOF

      # 5. Commit and push RSS feed
      - name: Commit and push RSS feed
        uses: ad-m/github-push-action@v0.6.0
        with:
          branch: main
          github_token: ${{ secrets.GITHUB_TOKEN }}
          commit_message: "Update RSS feed"
